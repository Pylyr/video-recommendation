{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer, MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import json\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2e5\n",
    "# nrows=1e9\n",
    "ratings = pd.read_csv('data/ratings.csv', nrows=nrows) \\\n",
    "        .drop('timestamp', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/newlife1/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:3526: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/newlife1/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/merge.py:1212: RuntimeWarning: invalid value encountered in cast\n",
      "  if not (rk == rk.astype(lk.dtype))[~np.isnan(rk)].all():\n"
     ]
    }
   ],
   "source": [
    "metadata = pd.read_csv('data/movies_metadata.csv') \\\n",
    "        .drop([\"homepage\", \"imdb_id\", \"poster_path\", \"production_countries\", \"status\", \"tagline\", \"original_title\", \"video\"], axis=1)   \n",
    "\n",
    "# we have to cast the id to int, so that we can merge the dataframes\n",
    "metadata = metadata.assign(\n",
    "    id=metadata['id'].apply(pd.to_numeric, errors='coerce').dropna().astype(int))\n",
    "ratings_with_metadata = ratings.merge(\n",
    "    metadata, left_on='movieId', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a random split of ratings\n",
    "train, test = train_test_split(ratings, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.LongTensor(train[['userId', 'movieId']].to_numpy())\n",
    "X_test = torch.LongTensor(test[['userId', 'movieId']].to_numpy())\n",
    "y_train = torch.Tensor(train['rating'].to_numpy())\n",
    "y_test = torch.Tensor(test['rating'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zero.als import MangakiALS\n",
    "from zero.svd import MangakiSVD\n",
    "\n",
    "svd = MangakiSVD()\n",
    "svd.nb_users = X_train[:, 0].unique().max().item() + 1\n",
    "svd.nb_works = X_train[:, 1].unique().max().item() + 1\n",
    "svd.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svd.predict(X_test)\n",
    "\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "als = MangakiALS()\n",
    "als.nb_users = X_train[:, 0].unique().max().item() + 1\n",
    "als.nb_works = X_train[:, 1].unique().max().item() + 1\n",
    "als.fit(X_train.detach().numpy(), y_train.detach().numpy())\n",
    "\n",
    "y_pred = als.predict(X_test.detach().numpy())\n",
    "\n",
    "print(mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(data):\n",
    "    return data.fillna('').astype(str)\n",
    "\n",
    "def convert_numeric(data):\n",
    "    return data.replace({'True': 1, 'False': 0}) \\\n",
    "               .apply(pd.to_numeric, errors='coerce').astype(float)\n",
    "\n",
    "\n",
    "def parse_genres(json_str):\n",
    "    try:\n",
    "        genres = json.loads(json_str.replace(\"'\", \"\\\"\"))\n",
    "        return [genre['name'] for genre in genres]\n",
    "    except json.JSONDecodeError:\n",
    "        return []\n",
    "\n",
    "def parse_json_df(data):\n",
    "    return data.apply(parse_genres).to_numpy()\n",
    "\n",
    "\n",
    "def binarize_genres(genres_list):\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    return mlb.fit_transform(genres_list)\n",
    "\n",
    "\n",
    "def date_transformer(data):\n",
    "    dates = pd.to_datetime(data.fillna('1900-01-01'),\n",
    "                           format=\"%Y-%m-%d\", errors='coerce')\n",
    "    return pd.DataFrame({\n",
    "        'release_year': dates.dt.year,\n",
    "        'release_month': dates.dt.month,\n",
    "        'release_day': dates.dt.day\n",
    "    })\n",
    "\n",
    "\n",
    "text_pipeline = Pipeline([\n",
    "    ('to_string', FunctionTransformer(preprocess_text, validate=False)),\n",
    "    ('vectorize', TfidfVectorizer(stop_words='english', max_features=1000))\n",
    "])\n",
    "\n",
    "numerical_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('to_float', FunctionTransformer(convert_numeric, validate=False)),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), ['budget', 'popularity', 'revenue', 'runtime', 'vote_count']),\n",
    "        ('passthrough', 'passthrough', ['rating']),\n",
    "        # ('text_overview', text_pipeline, 'overview'),\n",
    "\n",
    "        # This one is very inefficient\n",
    "        # ('json_genres', Pipeline([\n",
    "        #     ('extractor', FunctionTransformer(parse_json_df, validate=False)),\n",
    "        #     ('binarizer', FunctionTransformer(binarize_genres, validate=False))\n",
    "        # ]), 'genres'),\n",
    "        # ('release_date', FunctionTransformer(\n",
    "        #     date_transformer, validate=False), 'release_date'),\n",
    "\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = numerical_preprocessor.fit_transform(ratings_with_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an explainable decision tree model \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X, Y = numerical_data[:, :-1], numerical_data[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth=4, min_samples_split=1000, min_samples_leaf=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Change the index if you want to look at another tree\n",
    "tree = model\n",
    "\n",
    "# Visualize the tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "# Replace feature names with actual names\n",
    "plot_tree(tree, filled=True, max_depth=3, feature_names=[\n",
    "    'adult', 'budget', 'popularity', 'revenue', 'runtime', 'vote_count',\n",
    "    'release_year', 'release_month', 'release_day',\n",
    "    'userId', 'movieId'\n",
    "])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "os.environ['LIBFM_PATH'] = os.path.expanduser('~/libfm/bin/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), ['userId', 'movieId']),\n",
    "        ('passthrough', 'passthrough', ['rating']),\n",
    "\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "fm_data = fm_preprocessor.fit_transform(ratings_with_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywFM\n",
    "!source \"$HOME/.zshrc\"\n",
    "\n",
    "X, Y = fm_data[:, :-1], fm_data[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "fm = pywFM.FM(task='regression', num_iter=100,\n",
    "              init_stdev=0.1, learning_method='mcmc')\n",
    "\n",
    "prediction, global_bias, weights, pairwise, rlog = fm.run(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historical_ratings_encoder(df):\n",
    "    # Creating a copy to avoid changing original dataframe\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # One-hot encode userId and movieId within the copy for transformation\n",
    "    transformed_data = pd.get_dummies(df_copy, columns=['movieId'])\n",
    "\n",
    "    # Aggregate historical rated movies data for each user\n",
    "    filtered_movies = transformed_data.filter(regex='movieId_.*')\n",
    "    historical_rated_data = filtered_movies.groupby(df_copy['userId']).sum()\n",
    "    historical_rated_data = historical_rated_data.rename(\n",
    "        columns=lambda x: 'rated:' + x)\n",
    "\n",
    "    # Ensure historical data index is named for proper merging\n",
    "    historical_rated_data.index.name = 'userId'\n",
    "\n",
    "    # Resetting index to enable join on 'userId'\n",
    "    df_copy = df_copy.join(historical_rated_data, on='userId')\n",
    "\n",
    "    # Dropping duplicate or unnecessary columns\n",
    "    # Return only historical data columns\n",
    "    return df_copy.iloc[:, -filtered_movies.shape[1]:]\n",
    "\n",
    "\n",
    "fm_preprocessor_full = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), ['userId', 'movieId']),\n",
    "        ('num', Pipeline([\n",
    "            ('to_float', FunctionTransformer(convert_numeric, validate=False)),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), ['budget', 'popularity', 'revenue', 'runtime', 'vote_count']),\n",
    "        ('json_genres', Pipeline([\n",
    "            ('extractor', FunctionTransformer(parse_json_df, validate=False)),\n",
    "            ('binarizer', FunctionTransformer(binarize_genres, validate=False))\n",
    "        ]), 'genres'),\n",
    "        ('historical', FunctionTransformer(historical_ratings_encoder), ['userId', 'movieId']),\n",
    "        ('passthrough', 'passthrough', ['rating']),\n",
    "\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "fm_data_full = fm_preprocessor_full.fit_transform(ratings_with_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = fm_data_full[:, :-1], fm_data_full[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "fm = pywFM.FM(task='regression', num_iter=100,\n",
    "              init_stdev=0.1, learning_method='mcmc')\n",
    "\n",
    "prediction, global_bias, weights, pairwise, rlog = fm.run(\n",
    "    X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "# (43797, 3610) - raw 100 epochs 0.86\n",
    "# (43797, 6244) - history + genre info 100 epochs 0.88\n",
    "# (43797, 6230) - history + numericals 100 epochs 0.86\n",
    "# (43797, X)    - history + genre + numericals 100 epochs 0.87\n",
    "\n",
    "# \n",
    "# (437491, 20121) - history + numericals 100 epochs 0.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "deep_fm_data = ratings_with_metadata.drop(\n",
    "    ['id', 'original_language', 'overview', 'title', 'belongs_to_collection', 'revenue', 'budget', 'adult'], axis=1)\n",
    "\n",
    "deep_fm_data['genres'] = deep_fm_data['genres'].apply(parse_genres)\n",
    "deep_fm_data['spoken_languages'] = deep_fm_data['spoken_languages'].apply(\n",
    "    parse_genres)\n",
    "deep_fm_data['production_companies'] = deep_fm_data['production_companies'].apply(\n",
    "    parse_genres)\n",
    "\n",
    "# deep_fm_data['genres'] = deep_fm_data['genres'].apply(lambda x: ', '.join(x))\n",
    "# deep_fm_data['production_companies'] = deep_fm_data['production_companies'].apply(\n",
    "#     lambda x: ', '.join(x))\n",
    "# deep_fm_data['spoken_languages'] = deep_fm_data['spoken_languages'].apply(\n",
    "#     lambda x: ', '.join(x))\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['popularity', 'vote_average', 'vote_count', 'runtime']\n",
    "deep_fm_data[numerical_cols] = scaler.fit_transform(\n",
    "    deep_fm_data[numerical_cols])\n",
    "\n",
    "# Convert release date to just the year and treat as categorical\n",
    "deep_fm_data['release_year'] = pd.to_datetime(\n",
    "    deep_fm_data['release_date']).dt.year.fillna(1900).astype(float)\n",
    "\n",
    "deep_fm_data.drop(['release_date'], axis=1, inplace=True)\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "# fix nan in release_year, popularity and runtime\n",
    "deep_fm_data['release_year'] = imputer.fit_transform(\n",
    "    deep_fm_data['release_year'].values.reshape(-1, 1))\n",
    "deep_fm_data['popularity'] = imputer.fit_transform(\n",
    "    deep_fm_data['popularity'].values.reshape(-1, 1))\n",
    "deep_fm_data['runtime'] = imputer.fit_transform(\n",
    "    deep_fm_data['runtime'].values.reshape(-1, 1))\n",
    "\n",
    " \n",
    "\n",
    "# label_encoders = {}\n",
    "# for col in ['genres', 'production_companies', 'spoken_languages']:\n",
    "#     le = LabelEncoder()\n",
    "#     # Convert list to comma-separated string if not already\n",
    "#     deep_fm_data[col] = deep_fm_data[col].apply(lambda x: ','.join(x))\n",
    "#     deep_fm_data[col] = le.fit_transform(deep_fm_data[col])\n",
    "#     label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, VarLenSparseFeat, get_feature_names\n",
    "from keras.utils import pad_sequences\n",
    "from deepctr_torch.models import DeepFM\n",
    "\n",
    "\n",
    "def split(x):\n",
    "    key_ans = x\n",
    "    for key in key_ans:\n",
    "        if key not in key2index:\n",
    "            # Notice : input value 0 is a special \"padding\",so we do not use 0 to encode valid feature for sequence input\n",
    "            key2index[key] = len(key2index) + 1\n",
    "    return list(map(lambda x: key2index[x], key_ans))\n",
    "\n",
    "\n",
    "sparse_features = [\"movieId\", \"userId\"]\n",
    "list_features = ['genres', 'production_companies', 'spoken_languages']\n",
    "dense_features = [\"runtime\", \"vote_average\", \"popularity\", \"vote_count\", \"release_year\"]\n",
    "target = ['rating']\n",
    "\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    deep_fm_data[feat] = lbe.fit_transform(deep_fm_data[feat])\n",
    "\n",
    "model_input = {name: deep_fm_data[name] for name in sparse_features + dense_features}\n",
    "\n",
    "varlen_feature_columns = []\n",
    "for list_feature in list_features:\n",
    "    key2index = {}\n",
    "    genres_list = list(map(split, deep_fm_data[list_feature].values))\n",
    "    genres_length = np.array(list(map(len, genres_list)))\n",
    "    max_len = max(genres_length)\n",
    "    # Notice : padding=`post`\n",
    "    genres_list = pad_sequences(genres_list, maxlen=max_len, padding='post', )\n",
    "    model_input[list_feature] = genres_list\n",
    "    varlen_feature_columns.append(VarLenSparseFeat(SparseFeat(list_feature, vocabulary_size=len(\n",
    "    key2index) + 1, embedding_dim=4), maxlen=max_len, combiner='mean'))\n",
    "\n",
    "# 2.count #unique features for each sparse field\n",
    "fixlen_feature_columns = [SparseFeat(feat, deep_fm_data[feat].nunique() + 1, embedding_dim=4)\n",
    "                              for feat in sparse_features] + [DenseFeat(feat, 1, )\n",
    "                                                              for feat in dense_features]\n",
    "\n",
    "linear_feature_columns = fixlen_feature_columns + varlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns + varlen_feature_columns\n",
    "\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Train on 69571 samples, validate on 17393 samples, 272 steps per epoch\n",
      "Epoch 1/10\n",
      "5s - loss:  1.1270 - mse:  1.1086 - val_mse:  1.1287\n",
      "Epoch 2/10\n",
      "5s - loss:  0.9391 - mse:  0.8804 - val_mse:  1.0784\n",
      "Epoch 3/10\n",
      "4s - loss:  0.9326 - mse:  0.8580 - val_mse:  1.0684\n",
      "Epoch 4/10\n",
      "3s - loss:  0.9258 - mse:  0.8455 - val_mse:  1.0901\n",
      "Epoch 5/10\n",
      "3s - loss:  0.9310 - mse:  0.8474 - val_mse:  1.1037\n",
      "Epoch 6/10\n",
      "4s - loss:  0.9359 - mse:  0.8521 - val_mse:  1.1777\n",
      "Epoch 7/10\n",
      "5s - loss:  0.9300 - mse:  0.8446 - val_mse:  1.0727\n",
      "Epoch 8/10\n",
      "4s - loss:  0.9185 - mse:  0.8328 - val_mse:  1.0602\n",
      "Epoch 9/10\n",
      "4s - loss:  0.9248 - mse:  0.8388 - val_mse:  1.0602\n",
      "Epoch 10/10\n",
      "4s - loss:  0.9230 - mse:  0.8365 - val_mse:  1.0994\n"
     ]
    }
   ],
   "source": [
    "# 4.Define Model,train,predict and evaluate\n",
    "device = 'cpu'\n",
    "use_cuda = True\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    print('cuda ready...')\n",
    "    device = 'cuda:0'\n",
    "\n",
    "model = DeepFM(linear_feature_columns, dnn_feature_columns,\n",
    "               task='regression', device=device, \n",
    "               dnn_hidden_units=(64, 64),\n",
    "               l2_reg_dnn=1e-0,\n",
    "               l2_reg_embedding=1e-0,\n",
    "               l2_reg_linear=1e-0,\n",
    "               dnn_dropout=0.2)\n",
    "\n",
    "model.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "history = model.fit(model_input, deep_fm_data[target].values, batch_size=256,epochs=10,verbose=2,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
